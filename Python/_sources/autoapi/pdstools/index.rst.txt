pdstools
========

.. py:module:: pdstools

.. autoapi-nested-parse::

   Python pdstools



Subpackages
-----------

.. toctree::
   :maxdepth: 1

   /autoapi/pdstools/adm/index
   /autoapi/pdstools/app/index
   /autoapi/pdstools/decision_analyzer/index
   /autoapi/pdstools/ih/index
   /autoapi/pdstools/pega_io/index
   /autoapi/pdstools/plots/index
   /autoapi/pdstools/prediction/index
   /autoapi/pdstools/reports/index
   /autoapi/pdstools/utils/index
   /autoapi/pdstools/valuefinder/index


Attributes
----------

.. autoapisummary::

   pdstools.__version__
   pdstools.__reports__


Classes
-------

.. autoapisummary::

   pdstools.ADMDatamart
   pdstools.ADMTrees
   pdstools.MultiTrees
   pdstools.BinAggregator
   pdstools.CDHLimits
   pdstools.Config
   pdstools.DataAnonymization
   pdstools.PegaDefaultTables
   pdstools.ValueFinder
   pdstools.Sample
   pdstools.DecisionData


Functions
---------

.. autoapisummary::

   pdstools.get_token
   pdstools.readDSExport
   pdstools.setupAzureOpenAI
   pdstools.defaultPredictorCategorization
   pdstools.show_versions
   pdstools.CDHSample
   pdstools.SampleTrees
   pdstools.SampleValueFinder


Package Contents
----------------

.. py:data:: __version__
   :value: '3.4.5'


.. py:class:: ADMDatamart(path: Union[str, pathlib.Path] = Path('.'), import_strategy: Literal['eager', 'lazy'] = 'eager', *, model_filename: Optional[str] = 'modelData', predictor_filename: Optional[str] = 'predictorData', model_df: Optional[pdstools.utils.types.any_frame] = None, predictor_df: Optional[pdstools.utils.types.any_frame] = None, query: Optional[Union[polars.Expr, List[polars.Expr], str, Dict[str, list]]] = None, subset: bool = True, drop_cols: Optional[list] = None, include_cols: Optional[list] = None, context_keys: list = ['Channel', 'Direction', 'Issue', 'Group'], extract_keys: bool = False, predictorCategorization: polars.Expr = cdh_utils.defaultPredictorCategorization, plotting_engine: Union[str, Any] = 'plotly', verbose: bool = False, **reading_opts)

   Bases: :py:obj:`pdstools.plots.plot_base.Plots`, :py:obj:`pdstools.adm.Tables.Tables`


   Main class for importing, preprocessing and structuring Pega ADM Datamart.
   Gets all available data, properly names and merges into one main dataframe.

   It's also possible to import directly from S3. Please refer to
   :meth:`pdstools.pega_io.S3.S3Data.get_ADMDatamart`.

   :param path: The path of the data files
   :type path: str, default = "."
   :param import_strategy: Whether to import the file fully to memory, or scan the file
                           When data fits into memory, 'eager' is typically more efficient
                           However, when data does not fit, the lazy methods typically allow
                           you to still use the data.
   :type import_strategy: Literal['eager', 'lazy'], default = 'eager'

   :keyword model_filename: The name, or extended filepath, towards the model file
   :kwtype model_filename: Optional[str]
   :keyword predictor_filename: The name, or extended filepath, towards the predictors file
   :kwtype predictor_filename: Optional[str]
   :keyword model_df: Optional override to supply a dataframe instead of a file
   :kwtype model_df: Union[pl.DataFrame, pl.LazyFrame, pd.DataFrame]
   :keyword predictor_df: Optional override to supply a dataframe instead of a file
   :kwtype predictor_df: Union[pl.DataFrame, pl.LazyFrame, pd.DataFrame]
   :keyword query: Please refer to :meth:`._apply_query`
   :kwtype query: Union[pl.Expr, str, Dict[str, list]], default = None
   :keyword plotting_engine: Please refer to :meth:`.get_engine`
   :kwtype plotting_engine: str, default = "plotly"
   :keyword subset: Whether to only keep a subset of columns for efficiency purposes
                    Refer to :meth:`._available_columns` for the default list of columns.
   :kwtype subset: bool, default = True
   :keyword drop_cols: Columns to exclude from reading
   :kwtype drop_cols: Optional[list]
   :keyword include_cols: Additionial columns to include when reading
   :kwtype include_cols: Optional[list]
   :keyword context_keys: Which columns to use as context keys
   :kwtype context_keys: list, default = ["Channel", "Direction", "Issue", "Group"]
   :keyword extract_keys: Extra keys, particularly pyTreatment, are hidden within the pyName column.
                          extract_keys can expand that cell to also show these values.
                          To extract these extra keys, set extract_keys to True.
   :kwtype extract_keys: bool, default = False
   :keyword verbose: Whether to print out information during importing
   :kwtype verbose: bool, default = False
   :keyword \*\*reading_opts: Additional parameters used while reading.
                              Refer to :meth:`pdstools.pega_io.File.import_file` for more info.

   .. attribute:: modelData

      If available, holds the preprocessed data about the models

      :type: pl.LazyFrame

   .. attribute:: predictorData

      If available, holds the preprocessed data about the predictor binning

      :type: pl.LazyFrame

   .. attribute:: combinedData

      If both modelData and predictorData are available,
      holds the merged data about the models and predictors

      :type: pl.LazyFrame

   .. attribute:: import_strategy

      See the `import_strategy` parameter

   .. attribute:: query

      See the `query` parameter

   .. attribute:: context_keys

      See the `context_keys` parameter

   .. attribute:: verbose

      See the `verbose` parameter

   .. rubric:: Examples

   >>> Data =  ADMDatamart("/CDHSample")
   >>> Data =  ADMDatamart("Data/Adaptive Models & Predictors Export",
               model_filename = "Data-Decision-ADM-ModelSnapshot_AdaptiveModelSnapshotRepo20201110T085543_GMT/data.json",
               predictor_filename = "Data-Decision-ADM-PredictorBinningSnapshot_PredictorBinningSnapshotRepo20201110T084825_GMT/data.json")
   >>> Data =  ADMDatamart("Data/files",
               model_filename = "ModelData.csv",
               predictor_filename = "PredictorData.csv")


   .. py:attribute:: standardChannelGroups
      :value: ['Web', 'Mobile', 'E-mail', 'Push', 'SMS', 'Retail', 'Call Center', 'IVR']



   .. py:attribute:: standardDirections
      :value: ['Inbound', 'Outbound']



   .. py:attribute:: NBAD_model_configurations


   .. py:attribute:: import_strategy


   .. py:attribute:: context_keys


   .. py:attribute:: verbose


   .. py:attribute:: query


   .. py:attribute:: predictorCategorization


   .. py:attribute:: plotting_engine


   .. py:method:: get_engine(plotting_engine)
      :staticmethod:


      Which engine to use for creating the plots.

      By supplying a custom class here, you can re-use the pdstools functions
      but create visualisations to your own specifications, in any library.



   .. py:method:: import_data(path: Optional[Union[str, pathlib.Path]] = Path('.'), *, model_filename: Optional[str] = 'modelData', predictor_filename: Optional[str] = 'predictorData', model_df: Optional[pdstools.utils.types.any_frame] = None, predictor_df: Optional[pdstools.utils.types.any_frame] = None, subset: bool = True, drop_cols: Optional[list] = None, include_cols: Optional[list] = None, extract_keys: bool = False, verbose: bool = False, **reading_opts) -> Tuple[Optional[polars.LazyFrame], Optional[polars.LazyFrame]]

      Method to import & format the relevant data.

      The method first imports the model data, and then the predictor data.
      If model_df or predictor_df is supplied, it will use those instead
      If any filters are included in the the `query` argument of the ADMDatmart,
      those will be applied to the modeldata, and the predictordata will be
      filtered such that it only contains the model_ids leftover after filtering.
      After reading, some additional values (such as success rate) are
      automatically computed.
      Lastly, if there are missing columns from both datasets,
      this will be printed to the user if verbose is True.

      :param path: The path of the data files
                   Default = current path ('.')
      :type path: Path
      :param subset: Whether to only select the renamed columns,
                     set to False to keep all columns
      :type subset: bool, default = True
      :param model_df: Optional override to supply a dataframe instead of a file
      :type model_df: pd.DataFrame
      :param predictor_df: Optional override to supply a dataframe instead of a file
      :type predictor_df: pd.DataFrame
      :param drop_cols: Columns to exclude from reading
      :type drop_cols: Optional[list]
      :param include_cols: Additionial columns to include when reading
      :type include_cols: Optional[list]
      :param extract_keys: Extra keys, particularly pyTreatment, are hidden within the pyName column.
                           extract_keys can expand that cell to also show these values.
                           To extract these extra keys, set extract_keys to True.
      :type extract_keys: bool, default = False
      :param verbose: Whether to print out information during importing
      :type verbose: bool, default = False

      :returns: The model data and predictor binning data as LazyFrames
      :rtype: (polars.LazyFrame, polars.LazyFrame)



   .. py:property:: is_available
      :type: bool



   .. py:method:: _import_utils(name: Union[str, pdstools.utils.types.any_frame], path: Optional[str] = None, *, subset: bool = True, extract_keys: bool = False, drop_cols: Optional[list] = None, include_cols: Optional[list] = None, **reading_opts) -> Tuple[polars.LazyFrame, dict, dict]

      Handler function to interface to the cdh_utils methods

      :param name: One of {modelData, predictorData}
                   or a dataframe
      :type name: Union[str, pl.DataFrame]
      :param path: The path of the data file
      :type path: str, default = None

      :keyword subset: Whether to only select the renamed columns,
                       set to False to keep all columns
      :kwtype subset: bool, default = True
      :keyword drop_cols: Supply columns to drop from the dataframe
      :kwtype drop_cols: list
      :keyword include_cols: Supply columns to include with the dataframe
      :kwtype include_cols: list
      :keyword extract_keys: Treatments are typically hidden within the pyName column,
                             extract_keys can expand that cell to also show these values.
      :kwtype extract_keys: bool
      :keyword Additional keyword arguments:
      :keyword ----------------------------:
      :keyword See :meth:`pdstools.pega_io.File.readDSExport`:

      :returns:

                - The requested dataframe,
                - The renamed columns
                - The columns missing in both dataframes)
      :rtype: (pl.LazyFrame, dict, dict)



   .. py:method:: _available_columns(df: polars.LazyFrame, include_cols: Optional[list] = None, drop_cols: Optional[list] = None) -> Tuple[set, set]

      Based on the default names for variables, rename available data to proper formatting

      :param df: Input dataframe
      :type df: pl.LazyFrame
      :param include_cols: Supply columns to include with the dataframe
      :type include_cols: list
      :param drop_cols: Supply columns to not import at all
      :type drop_cols: list

      :returns: The original dataframe, but renamed for the found columns &
                The original and updated names for all renamed columns &
                The variables that were not found in the table
      :rtype: Tuple[set, set]



   .. py:method:: _set_types(df: pdstools.utils.types.any_frame, table: str = 'infer', *, timestamp_fmt: str = None, strict_conversion: bool = True) -> pdstools.utils.types.any_frame

      A method to change columns to their proper type

      :param df: The input dataframe
      :type df: Union[pl.DataFrame, pl.LazyFrame]
      :param table: The table to set types for. Default is infer, in which case
                    it infers the table type from the columns in it.
      :type table: str

      :keyword timestamp_fmt: The format of Date type columns
      :kwtype timestamp_fmt: str
      :keyword strict_conversion: Raises an error if timestamp conversion to given/default date format(timestamp_fmt) fails
                                  See 'https://strftime.org/' for timestamp formats
      :kwtype strict_conversion: bool

      :returns: The input dataframe, but the proper typing applied
      :rtype: Union[pl.DataFrame, pl.LazyFrame]



   .. py:method:: last(table='modelData', strategy: Literal['eager', 'lazy'] = 'eager') -> pdstools.utils.types.any_frame

      Convenience function to get the last values for a table

      :param table: Which table to get the last values for
                    One of {modelData, predictorData, combinedData}
      :type table: str, default = modelData
      :param strategy: Whether to import the file fully to memory, or scan the file
                       When data fits into memory, 'eager' is typically more efficient
                       However, when data does not fit, the lazy methods typically allow
                       you to still use the data.
      :type strategy: Literal['eager', 'lazy'], default = 'eager'

      :returns: The last snapshot for each model
      :rtype: Union[pl.DataFrame, pl.LazyFrame]



   .. py:method:: _last(df: pdstools.utils.types.any_frame) -> pdstools.utils.types.any_frame
      :staticmethod:


      Method to retrieve only the last snapshot.



   .. py:method:: _last_timestamp(col: Literal['ResponseCount', 'Positives']) -> polars.Expr
      :staticmethod:


      Add a column to indicate the last timestamp a column has changed.

      :param col: The column to calculate the diff for
      :type col: Literal['ResponseCount', 'Positives']



   .. py:method:: _get_combined_data(last=True, strategy: Literal['eager', 'lazy'] = 'eager') -> pdstools.utils.types.any_frame

      Combines the model data and predictor data into one dataframe.

      :param last: Whether to only use the last snapshot for each table
      :type last: bool, default=True
      :param strategy: Whether to import the file fully to memory, or scan the file
                       When data fits into memory, 'eager' is typically more efficient
                       However, when data does not fit, the lazy methods typically allow
                       you to still use the data.
      :type strategy: Literal['eager', 'lazy'], default = 'eager'

      :returns: The combined dataframe
      :rtype: Union[pl.DataFrame, pl.LazyFrame]



   .. py:method:: processTables(query: Optional[Union[polars.Expr, List[polars.Expr], str, Dict[str, list]]] = None) -> ADMDatamart

      Processes modelData, predictorData and combinedData tables.

      Can take in a query, which it will apply to modelData
      If a query is given, it joins predictorData to only retain the modelIDs
      the modelData was filtered on. If both modelData and predictorData
      are present, it joins them together into combinedData.

      If memory_strategy is eager, which is the default, this method also
      collects the tables and then sets them back to lazy.

      :param query: An optional query to apply to the modelData table.
                    See: :meth:`._apply_query`
      :type query: Optional[Union[pl.Expr, List[pl.Expr], str, Dict[str, list]]], default = None



   .. py:method:: save_data(path: str = '.') -> Tuple[os.PathLike, os.PathLike]

      Cache modelData and predictorData to files.

      :param path: Where to place the files
      :type path: str

      :returns: The paths to the model and predictor data files
      :rtype: (os.PathLike, os.PathLike)



   .. py:method:: _apply_query(df: pdstools.utils.types.any_frame, query: Optional[Union[polars.Expr, List[polars.Expr], str, Dict[str, list]]] = None) -> polars.LazyFrame

      Given an input Polars dataframe, it filters the dataframe based on input query

      :param df: The input dataframe
      :type df: Union[pl.DataFrame, pl.LazyFrame]
      :param query: If a Polars Expression, passes the expression into Polars' filter function.
                    If a list of Polars Expressions, applies each of the expressions as filters.
                    If a string, uses the Pandas query function (works only in eager mode,
                    not recommended).
                    Else, a dict of lists where the key is column name in the dataframe
                    and the corresponding value is a list of values to keep in the dataframe
      :type query: Optional[Union[pl.Expr, List[pl.Expr], str, Dict[str, list]]]

      :returns: Filtered Polars DataFrame
      :rtype: pl.DataFrame



   .. py:method:: discover_modelTypes(df: polars.LazyFrame, by: str = 'Configuration', allow_collect=False) -> Dict

      Discovers the type of model embedded in the pyModelData column.

      By default, we do a group_by Configuration, because a model rule can only
      contain one type of model. Then, for each configuration, we look into the
      pyModelData blob and find the _serialClass, returning it in a dict.

      :param df: The dataframe to search for model types
      :type df: pl.LazyFrame
      :param by: The column to look for types in. Configuration is recommended.
      :type by: str
      :param allow_collect: Set to True to allow discovering modelTypes, even if in lazy strategy.
                            It will fetch one modelData string per configuration.
      :type allow_collect: bool, default = False



   .. py:method:: get_AGB_models(last: bool = False, by: str = 'Configuration', n_threads: int = 1, query: Optional[Union[polars.Expr, List[polars.Expr], str, Dict[str, list]]] = None, verbose: bool = True, **kwargs) -> Dict

      Method to automatically extract AGB models.

      Recommended to subset using the querying functionality
      to cut down on execution time, because it checks for each
      model ID. If you only have AGB models remaining after the query,
      it will only return proper AGB models.

      :param last: Whether to only look at the last snapshot for each model
      :type last: bool, default = False
      :param by: Which column to determine unique models with
      :type by: str, default = 'Configuration'
      :param n_threads: The number of threads to use for extracting the models.
                        Since we use multithreading, setting this to a reasonable value
                        helps speed up the import.
      :type n_threads: int, default = 6
      :param query: Please refer to :meth:`._apply_query`
      :type query: Optional[Union[pl.Expr, List[pl.Expr], str, Dict[str, list]]]
      :param verbose: Whether to print out information while importing
      :type verbose: bool, default = False



   .. py:method:: _create_sign_df(df: polars.LazyFrame, by: str = 'Name', *, what: str = 'ResponseCount', every: str = '1d', pivot: bool = True, mask: bool = True) -> polars.LazyFrame
      :staticmethod:


      Generates dataframe to show whether responses decreased/increased from day to day

      For a given dataframe where columns are dates and rows are model names(by parameter),
      subtracts each day's value from the previous day's value per model. Then masks the data.
      If increased (desired situtation), it will put 1 in the cell, if no change, it will
      put 0, and if decreased it will put -1. This dataframe then could be used in the heatmap

      :param df: This is typically pivoted ModelData
      :type df: pd.DataFrame
      :param by: Column to calculate the daily change for.
      :type by: str, default = Name

      :keyword what: Column that contains response counts
      :kwtype what: str, default = ResponseCount
      :keyword every: Interval of the change window
      :kwtype every: str, default = 1d
      :keyword pivot: Returns a pivotted table with signs as value if set to true
      :kwtype pivot: bool, default = True
      :keyword mask: Drops SnapshotTime and returns direction of change(sign).
      :kwtype mask: bool, default = True

      :returns: The dataframe with signs for increase or decrease in day to day
      :rtype: pd.LazyFrame



   .. py:method:: model_summary(by: str = 'ModelID', query: Optional[Union[polars.Expr, List[polars.Expr], str, Dict[str, list]]] = None, **kwargs) -> polars.LazyFrame

      Convenience method to automatically generate a summary over models

      By default, it summarizes ResponseCount, Performance, SuccessRate & Positives by model ID.
      It also adds weighted means for Performance and SuccessRate,
      And adds the count of models without responses and the percentage.

      :param by: By what column to summarize the models
      :type by: str, default = ModelID
      :param query: Please refer to :meth:`._apply_query`
      :type query: Optional[Union[pl.Expr, List[pl.Expr], str, Dict[str, list]]]

      :returns: group_by dataframe over all models
      :rtype: pl.LazyFrame



   .. py:method:: pivot_df(df: polars.LazyFrame, by: Union[str, list] = 'Name', *, allow_collect: bool = True, top_n: int = 0) -> polars.DataFrame

      Simple function to extract pivoted information

      :param df: The input DataFrame.
      :type df: pl.LazyFrame
      :param by: The column(s) to pivot the DataFrame by.
                 If a list is provided, only the first element is used.
      :type by: Union[str, list], default = Name
      :param allow_collect: Whether to allow eager computation.
                            If set to False and the import strategy is "lazy", an error will be raised.
      :type allow_collect: bool, default = True
      :param top_n: The number of rows to include in the pivoted DataFrame.
                    If set to 0, all rows are included.
      :type top_n: int, optional (default=0)

      :returns: The pivoted DataFrame.
      :rtype: pl.DataFrame



   .. py:method:: response_gain_df(df: pdstools.utils.types.any_frame, by: str = 'Channel') -> pdstools.utils.types.any_frame
      :staticmethod:


      Simple function to extract the response gain per model



   .. py:method:: models_by_positives_df(df: polars.LazyFrame, by: str = 'Channel', allow_collect=True) -> polars.LazyFrame

      Compute statistics on the dataframe by grouping it by a given column `by`
      and computing the count of unique ModelIDs and cumulative percentage of unique
      models for with regard to the number of positive answers.

      :param df: The input DataFrame
      :type df: pl.LazyFrame
      :param by: The column name to group the DataFrame by, by default "Channel"
      :type by: str, default = Channel
      :param allow_collect: Whether to allow eager computation. If set to False and the import strategy is "lazy", an error will be raised.
      :type allow_collect: bool, default = True

      :returns: DataFrame with PositivesBin column and model count statistics
      :rtype: pl.LazyFrame



   .. py:method:: get_model_stats(last: bool = True) -> dict

      Returns a dictionary containing various statistics for the model data.

      :param last: Whether to compute statistics only on the last snapshot. Defaults to True.
      :type last: bool

      :returns: A dictionary containing the following keys:
                'models_n_snapshots': The number of distinct snapshot times in the data.
                'models_total': The total number of models in the data.
                'models_empty': The models with no responses.
                'models_nopositives': The models with responses but no positive responses.
                'models_isimmature': The models with less than 200 positive responses.
                'models_noperformance': The models with at least 200 positive responses but a performance of 50.
                'models_n_nonperforming': The total number of models that are not performing well.
                'models_missing_{key}': The number of models with missing values for each context key.
                'models_bottom_left': The models with a performance of 50 and a success rate of 0.
      :rtype: Dict



   .. py:method:: describe_models(**kwargs) -> NoReturn

      Convenience method to quickly summarize the models



   .. py:method:: applyGlobalQuery(query: Union[polars.Expr, List[polars.Expr], str, Dict[str, list]]) -> ADMDatamart

      Convenience method to further query the datamart

      It's possible to give this query to the initial `ADMDatamart` class
      directly, but this method is more explicit. Filters on the model data
      (query is put in a :meth:`polars.filter()` method), filters the predictorData
      on the ModelIDs remaining after the query, and recomputes combinedData.

      Only works with Polars expressions.

      Paramters
      ---------
      query: Union[pl.Expr, List[pl.Expr], str, Dict[str, list]]
          The query to apply, see :meth:`._apply_query`



   .. py:method:: fillMissing() -> ADMDatamart

      Convenience method to fill missing values

      - Fills categorical, string and null type columns with "NA"
      - Fills SuccessRate, Performance and ResponseCount columns with 0
      - When context keys have empty string values, replaces them
      with "NA" string



   .. py:method:: summary_by_channel(custom_channels: Dict[str, str] = None, by_period: str = None, keep_lists: bool = False) -> polars.LazyFrame

      Summarize ADM models per channel

      :param custom_channels: Optional list with custom channel/direction name mappings. Defaults to None.
      :type custom_channels: Dict[str, str], optional
      :param by_period: Optional grouping by time period. Format string as in polars.Expr.dt.truncate (https://docs.pola.rs/api/python/stable/reference/expressions/api/polars.Expr.dt.truncate.html), for example "1mo", "1w", "1d" for calendar month, week day. If provided, creates a new Period column with the truncated date/time. Defaults to None.
      :type by_period: str, optional
      :param keep_lists: Internal flag to keep some columns (action and treatment names etc) as full lists.
      :type keep_lists: bool, optional

      :returns: Dataframe with summary per channel (and optionally a period)
      :rtype: pl.LazyFrame



   .. py:method:: overall_summary(custom_channels: Dict[str, str] = None, by_period: str = None) -> polars.LazyFrame

      Overall ADM models summary. Only valid data is included.

      :param custom_channels: Optional list with custom channel/direction name mappings. Defaults to None.
      :type custom_channels: Dict[str, str], optional
      :param by_period: Optional grouping by time period. Format string as in polars.Expr.dt.truncate (https://docs.pola.rs/api/python/stable/reference/expressions/api/polars.Expr.dt.truncate.html), for example "1mo", "1w", "1d" for calendar month, week day. If provided, creates a new Period column with the truncated date/time. Defaults to None.
      :type by_period: str, optional

      :returns: Summary across all valid ADM models as a dataframe
      :rtype: pl.LazyFrame



   .. py:method:: generate_model_reports(name: Optional[str] = None, model_list: List[str] = None, working_dir: Optional[pathlib.Path] = None, only_active_predictors: bool = False, *, base_file_name: str = None, output_type: str = 'html', debug_mode: bool = False, progress_callback=None, **kwargs) -> pathlib.Path

      Generates model reports.

      :param name: The name of the report.
      :type name: str, optional
      :param model_list: The list of model IDs to generate reports for.
      :type model_list: List[str]
      :param working_dir: The working directory for the output. If None, uses current working directory.
      :type working_dir: Path, optional
      :param only_active_predictors: Whether to only include active predictor details.
      :type only_active_predictors: bool, default=False
      :param base_file_name: The base file name for the generated reports. Defaults to None.
      :type base_file_name: str, optional
      :param output_type: The type of the output file (e.g., "html", "pdf").
      :type output_type: str, default='html'
      :param debug_mode: If True, the temporary directory will not be deleted after report generation.
      :type debug_mode: bool, optional
      :param \*\*kwargs: Additional keyword arguments.
      :type \*\*kwargs: dict

      :returns: The path to the generated report file.
      :rtype: Path

      :raises ValueError: If there's an error in report generation or invalid parameters.
      :raises FileNotFoundError: If required files are not found.
      :raises subprocess.SubprocessError: If there's an error in running external commands.



   .. py:method:: generate_health_check(name: Optional[str] = None, working_dir: Optional[pathlib.Path] = None, *, output_type: str = 'html', **kwargs) -> pathlib.Path

      Generates a report based on the provided parameters.

      :param name: The name of the report.
      :type name: str, optional
      :param working_dir: The working directory for the output. If None, uses current working directory.
      :type working_dir: Path, optional
      :param output_type: The type of the output file (e.g., "html", "pdf").
      :type output_type: str, default='html'
      :param debug_mode: If True, a temporary directory will be created and not deleted after report generation.
      :type debug_mode: bool, optional
      :param \*\*kwargs: Additional keyword arguments.
      :type \*\*kwargs: dict

      :returns: The path to the generated report file.
      :rtype: Path

      :raises ValueError: If there's an error in report generation or invalid parameters.
      :raises FileNotFoundError: If required files are not found.
      :raises subprocess.SubprocessError: If there's an error in running external commands.



   .. py:method:: _get_output_filename(name, report_type, model_id, output_type)

      Generate the output filename based on the report parameters.



   .. py:method:: _copy_quarto_file(qmd_file, temp_dir)

      Copy the report quarto file to the temporary directory.



   .. py:method:: _verify_cached_files(temp_dir)

      Verify that cached data files exist.



   .. py:method:: _write_params_file(temp_dir, model_id, only_active_predictors)

      Write parameters to a YAML file.



   .. py:method:: _run_quarto_command(temp_dir, qmd_file, output_type, output_filename, verbose)

      Run the Quarto command to generate the report.



   .. py:method:: _find_quarto_executable()

      Find the Quarto executable on the system.



   .. py:method:: exportTables(file: pathlib.Path = 'Tables.xlsx', predictorBinning=False)

      Exports all tables from `pdstools.adm.Tables` into one Excel file.

      :param file: The file name of the exported Excel file
      :type file: Path, default = 'Tables.xlsx'
      :param predictorBinning: If False, the 'predictorbinning' table will not be created
      :type predictorBinning: bool, default = True



.. py:class:: ADMTrees

   .. py:method:: getMultiTrees(file: polars.DataFrame, n_threads=1, verbose=True, **kwargs)
      :staticmethod:



.. py:class:: MultiTrees

   .. py:attribute:: trees
      :type:  dict


   .. py:attribute:: model_name
      :type:  str
      :value: None



   .. py:attribute:: context_keys
      :type:  list
      :value: None



   .. py:method:: __repr__()

      Return repr(self).



   .. py:method:: __getitem__(index)


   .. py:method:: __len__()


   .. py:method:: __add__(other)


   .. py:property:: first


   .. py:property:: last


   .. py:method:: computeOverTime(predictorCategorization=None)


   .. py:method:: plotSplitsPerVariableType(predictorCategorization=None, **kwargs)


.. py:class:: BinAggregator(dm: pdstools.ADMDatamart, query: polars.Expr = None)

   A class to generate rolled up insights from ADM predictor binning.


   .. py:attribute:: data


   .. py:attribute:: all_predictorbinning


   .. py:method:: roll_up(predictors: Union[str, list], n: int = 10, distribution: Literal['lin', 'log'] = 'lin', boundaries: Optional[Union[float, list]] = None, symbols: Optional[Union[str, list]] = None, minimum: Optional[float] = None, maximum: Optional[float] = None, aggregation: Optional[str] = None, as_numeric: Optional[bool] = None, return_df: bool = False, verbose: bool = False) -> Union[polars.DataFrame, plotly.graph_objects.Figure]

      Roll up a predictor across all the models defined when creating the class.

      Predictors can be both numeric and symbolic (also called 'categorical'). You
      can aggregate the same predictor across different sets of models by specifying
      a column name in the aggregation argument.

      :param predictors: Name of the predictor to roll up. Multiple predictors can be passed in as
                         a list.
      :type predictors: str | list
      :param n: Number of bins (intervals or symbols) to generate, by default 10. Any
                custom intervals or symbols specified with the 'musthave' argument will
                count towards this number as well. For symbolic predictors can be None,
                which means unlimited.
      :type n: int, optional
      :param distribution: For numeric predictors: the way the intervals are constructed. By default
                           "lin" for an evenly-spaced distribution, can be set to "log" for a long
                           tailed distribution (for fields like income).
      :type distribution: str, optional
      :param boundaries: For numeric predictors: one value, or a list of the numeric values to
                         include as interval boundaries. They will be used at the front of the
                         automatically created intervals. By default None, all intervals are
                         created automatically.
      :type boundaries: float | list, optional
      :param symbols: For symbolic predictors, any symbol(s) that
                      must be included in the symbol list in the generated binning. By default None.
      :type symbols: str | list, optional
      :param minimum: Minimum value for numeric predictors, by default None. When None the
                      minimum is taken from the binning data of the models.
      :type minimum: float, optional
      :param maximum: Maximum value for numeric predictors, by default None. When None the
                      maximum is taken from the binning data of the models.
      :type maximum: float, optional
      :param aggregation: Optional column name in the data to aggregate over, creating separate
                          aggregations for each of the different values. By default None.
      :type aggregation: str, optional
      :param as_numeric: Optional override for the type of the predictor, so to be able to
                         override in the (exceptional) situation that a predictor with the same
                         name is numeric in some and symbolic in some other models. By default None
                         which means the type is taken from the first predictor in the data.
      :type as_numeric: bool, optional
      :param return_df: Return the underlying binning instead of a plot.
      :type return_df: bool, optional
      :param verbose: Show detailed debug information while executing, by default False
      :type verbose: bool, optional

      :returns: By default returns a nicely formatted plot. When 'return_df' is set
                to True, it returns the actual binning with the lift aggregated over
                all the models, optionally per predictor and per set of models.
      :rtype: pl.DataFrame | Figure



   .. py:method:: accumulate_num_binnings(predictor, modelids, target_binning, verbose=False) -> polars.DataFrame


   .. py:method:: create_symbol_list(predictor, n_symbols, musthave_symbols) -> list


   .. py:method:: accumulate_sym_binnings(predictor, modelids, symbollist, verbose=False) -> polars.DataFrame


   .. py:method:: normalize_all_binnings(combined_dm: polars.LazyFrame) -> polars.LazyFrame

      Prepare all predictor binning

      Fix up the boundaries for numeric bins and parse the bin labels
      into clean lists for symbolics.



   .. py:method:: create_empty_numbinning(predictor: str, n: int, distribution: str = 'lin', boundaries: Optional[list] = None, minimum: Optional[float] = None, maximum: Optional[float] = None) -> polars.DataFrame


   .. py:method:: get_source_numbinning(predictor: str, modelid: str) -> polars.DataFrame


   .. py:method:: combine_two_numbinnings(source: polars.DataFrame, target: polars.DataFrame, verbose=False) -> polars.DataFrame


   .. py:method:: plot_binning_attribution(source: polars.DataFrame, target: polars.DataFrame) -> plotly.graph_objects.Figure


   .. py:method:: plotBinningLift(binning, col_facet=None, row_facet=None, custom_data=['PredictorName', 'BinSymbol'], return_df=False) -> Union[polars.DataFrame, plotly.graph_objects.Figure]


   .. py:method:: plot_lift_binning(binning: polars.DataFrame) -> plotly.graph_objects.Figure


.. py:function:: get_token(credentialFile: str, verify: bool = True, **kwargs)

   Get API credentials to a Pega Platform instance.

   After setting up OAuth2 authentication in Dev Studio, you should
   be able to download a credential file. Simply point this method to that file,
   and it'll read the relevant properties and give you your access token.

   :param credentialFile: The credential file downloaded after setting up OAuth in a Pega system
   :type credentialFile: str
   :param verify: Whether to only allow safe SSL requests.
                  In case you're connecting to an unsecured API endpoint, you need to
                  explicitly set verify to False, otherwise Python will yell at you.
   :type verify: bool, default = True

   :keyword url: An optional override of the URL to connect to.
                 This is also extracted out of the credential file, but you may want
                 to customize this (to a different port, etc).
   :kwtype url: str


.. py:function:: readDSExport(filename: Union[pandas.DataFrame, polars.DataFrame, str], path: str = '.', verbose: bool = True, **reading_opts) -> polars.LazyFrame

   Read a Pega dataset export file.
   Can accept either a Pandas DataFrame or one of the following formats:
   - .csv
   - .json
   - .zip (zipped json or CSV)
   - .feather
   - .ipc
   - .parquet

   It automatically infers the default file names for both model data as well as predictor data.
   If you supply either 'modelData' or 'predictorData' as the 'file' argument, it will search for them.
   If you supply the full name of the file in the 'path' directory, it will import that instead.
   Since pdstools V3.x, returns a Polars LazyFrame. Simply call `.collect()` to get an eager frame.

   :param filename: Either a Pandas/Polars DataFrame with the source data (for compatibility),
                    or a string, in which case it can either be:
                    - The name of the file (if a custom name) or
                    - Whether we want to look for 'modelData' or 'predictorData' in the path folder.
   :type filename: [pd.DataFrame, pl.DataFrame, str]
   :param path: The location of the file
   :type path: str, default = '.'
   :param verbose: Whether to print out which file will be imported
   :type verbose: bool, default = True

   :keyword Any: Any arguments to plug into the scan_* function from Polars.

   :returns: * *pl.LazyFrame* -- The (lazy) dataframe
             * *Examples* -- >>> df = readDSExport(filename = 'modelData', path = './datamart')
               >>> df = readDSExport(filename = 'ModelSnapshot.json', path = 'data/ADMData')

               >>> df = pd.read_csv('file.csv')
               >>> df = readDSExport(filename = df)


.. py:function:: setupAzureOpenAI(api_base: str = 'https://aze-openai-01.openai.azure.com/', api_version: Literal['2022-12-01', '2023-03-15-preview', '2023-05-15', '2023-06-01-preview', '2023-07-01-preview', '2023-09-15-preview', '2023-10-01-preview', '2023-12-01-preview'] = '2023-12-01-preview')

   Convenience function to automagically setup Azure AD-based authentication
   for the Azure OpenAI service. Mostly meant as an internal tool within Pega,
   but can of course also be used beyond.

   Prerequisites (you should only need to do this once!):
   - Download Azure CLI (https://learn.microsoft.com/en-us/cli/azure/install-azure-cli)
   - Once installed, run 'az login' in your terminal
   - Additional dependencies: `(pip install)` azure-identity and `(pip install)` openai

   Running this function automatically sets, among others:
   - `openai.api_key`
   - `os.environ["OPENAI_API_KEY"]`

   This should ensure that you don't need to pass tokens and/or api_keys around.
   The key that's set has a lifetime, typically of one hour. Therefore, if you
   get an error message like 'invalid token', you may need to run this method again
   to refresh the token for another hour.

   :param api_base: The url of the Azure service name you'd like to connect to
                    If you have access to the Azure OpenAI playground
                    (https://oai.azure.com/portal), you can easily find this url by clicking
                    'view code' in one of the playgrounds. If you have access to the Azure portal
                    directly (https://portal.azure.com), this will be found under 'endpoint'.
                    Else, ask your system administrator for the correct url.
   :type api_base: str
   :param api_version: The version of the api to use
   :type api_version: str
   :param Usage:
   :param -----:
   :param >>> from pdstools import setupAzureOpenAI:
   :param >>> setupAzureOpenAI():


.. py:class:: CDHLimits

   Bases: :py:obj:`object`


   A singleton container for best practice limits for CDH.

   Limits taken from https://docs-previous.pega.com/pega-customer-decision-hub-user-guide/87/service-and-data-health-limits-pega-customer-decision-hub-pega-cloud?
   but note these apply to Pega Cloud only.


   .. py:attribute:: Status


   .. py:attribute:: Metrics


   .. py:attribute:: _instance
      :value: None



   .. py:attribute:: num_limit


   .. py:attribute:: lims


   .. py:method:: get_limits(metric: Metrics) -> Union[num_limit, None]


   .. py:method:: check_limits(metric: Metrics, value: object) -> Status


.. py:function:: defaultPredictorCategorization(x: Union[str, polars.Expr] = pl.col('PredictorName')) -> polars.Expr

   Function to determine the 'category' of a predictor.

   It is possible to supply a custom function.
   This function can accept an optional column as input
   And as output should be a Polars expression.
   The most straight-forward way to implement this is with
   pl.when().then().otherwise(), which you can chain.

   By default, this function returns "Primary" whenever
   there is no '.' anywhere in the name string,
   otherwise returns the first string before the first period

   :param x: The column to parse
   :type x: Union[str, pl.Expr], default = pl.col('PredictorName')


.. py:function:: show_versions() -> None

   Print out version of pdstools and dependencies to stdout.

   .. rubric:: Examples

   >>> from pdstools import show_versions
   >>> show_versions()
   ---Version info---
   pdstools: 3.1.0
   Platform: macOS-12.6.4-x86_64-i386-64bit
   Python: 3.11.0 (v3.11.0:deaf509e8f, Oct 24 2022, 14:43:23) [Clang 13.0.0 (clang-1300.0.29.30)]
   ---Dependencies---
   plotly: 5.13.1
   requests: 2.28.1
   pydot: 1.4.2
   polars: 0.17.0
   pyarrow: 11.0.0.dev52
   tqdm: 4.64.1
   pyyaml: <not installed>
   aioboto3: 11.0.1
   ---Streamlit app dependencies---
   streamlit: 1.20.0
   quarto: 0.1.0
   papermill: 2.4.0
   itables: 1.5.1
   pandas: 1.5.3
   jinja2: 3.1.2
   xlsxwriter: 3.0


.. py:function:: CDHSample(plotting_engine='plotly', query=None, **kwargs)

.. py:function:: SampleTrees()

.. py:function:: SampleValueFinder(verbose=True)

.. py:class:: Config(config_file: Optional[str] = None, hds_folder: pathlib.Path = '.', use_datamart: bool = False, datamart_folder: pathlib.Path = 'datamart', output_format: Literal['ndjson', 'parquet', 'arrow', 'csv'] = 'ndjson', output_folder: pathlib.Path = 'output', mapping_file: str = 'mapping.map', mask_predictor_names: bool = True, mask_context_key_names: bool = False, mask_ih_names: bool = True, mask_outcome_name: bool = False, mask_predictor_values: bool = True, mask_context_key_values: bool = True, mask_ih_values: bool = True, mask_outcome_values: bool = True, context_key_label: str = 'Context_*', ih_label: str = 'IH_*', outcome_column: str = 'Decision_Outcome', positive_outcomes: list = ['Accepted', 'Clicked'], negative_outcomes: list = ['Rejected', 'Impression'], special_predictors: list = ['Decision_DecisionTime', 'Decision_OutcomeTime', 'Decision_Rank'], sample_percentage_schema_inferencing: float = 0.01)

   Configuration file for the data anonymizer.

   :param config_file: An optional path to a config file
   :type config_file: str = None
   :param hds_folder: The path to the hds files
   :type hds_folder: Path = "."
   :param use_datamart: Whether to use the datamart to infer predictor types
   :type use_datamart: bool = False
   :param datamart_folder: The folder of the datamart files
   :type datamart_folder: Path = "datamart"
   :param output_format: The output format to write the files in
   :type output_format: Literal["ndjson", "parquet", "arrow", "csv"] = "ndjson"
   :param output_folder: The location to write the files to
   :type output_folder: Path = "output"
   :param mapping_file: The name of the predictor mapping file
   :type mapping_file: str = "mapping.map"
   :param mask_predictor_names: Whether to mask the names of regular predictors
   :type mask_predictor_names: bool = True
   :param mask_context_key_names: Whether to mask the names of context key predictors
   :type mask_context_key_names: bool = True
   :param mask_ih_names: Whether to mask the name of Interaction History summary predictors
   :type mask_ih_names: bool = True
   :param mask_outcome_name: Whether to mask the name of the outcome column
   :type mask_outcome_name: bool = True
   :param mask_predictor_values: Whether to mask the values of regular predictors
   :type mask_predictor_values: bool = True
   :param mask_context_key_values: Whether to mask the values of context key predictors
   :type mask_context_key_values: bool = True
   :param mask_ih_values: Whether to mask the values of Interaction History summary predictors
   :type mask_ih_values: bool = True
   :param mask_outcome_values: Whether to mask the values of the outcomes to binary
   :type mask_outcome_values: bool = True
   :param context_key_label: The pattern of names for context key predictors
   :type context_key_label: str = "Context_*"
   :param ih_label: The pattern of names for Interaction History summary predictors
   :type ih_label: str = "IH_*"
   :param outcome_column: The name of the outcome column
   :type outcome_column: str = "Decision_Outcome"
   :param positive_outcomes: Which positive outcomes to map to True
   :type positive_outcomes: list = ["Accepted", "Clicked"]
   :param negative_outcomes: Which negative outcomes to map to False
   :type negative_outcomes: list = ["Rejected", "Impression"]
   :param special_predictors: A list of special predictors which are not touched
   :type special_predictors: list = ["Decision_DecisionTime", "Decision_OutcomeTime"]
   :param sample_percentage_schema_inferencing: The percentage of records to sample to infer the column type.
                                                In case you're getting casting errors, it may be useful to
                                                increase this percentage to check a larger portion of data.
   :type sample_percentage_schema_inferencing: float


   .. py:attribute:: _opts


   .. py:method:: load_from_config_file(config_file: pathlib.Path)

      Load the configurations from a file.

      :param config_file: The path to the configuration file
      :type config_file: Path



   .. py:method:: save_to_config_file(file_name: str = None)

      Save the configurations to a file.

      :param file_name: The name of the configuration file
      :type file_name: str



   .. py:method:: validate_paths()

      Validate the outcome folder exists.



.. py:class:: DataAnonymization(config: Optional[Config] = None, df: Optional[polars.LazyFrame] = None, datamart: Optional[pdstools.ADMDatamart] = None, **config_args)

   Anonymize a historical dataset.

   :param config: Override the default configurations with the Config class
   :param df: Manually supply a Polars lazyframe to anonymize
   :param datamart: Manually supply a Datamart file to infer predictor types

   :keyword \*\*config_args: See :Class:`.Config`

   .. rubric:: Example

   See https://pegasystems.github.io/pega-datascientist-tools/Python/articles/Example_Data_Anonymization.html


   .. py:attribute:: config


   .. py:attribute:: df_out
      :value: None



   .. py:method:: write_to_output(df: Optional[polars.DataFrame] = None, ext: Literal['ndjson', 'parquet', 'arrow', 'csv'] = None, mode: Literal['optimized', 'robust'] = 'optimized')

      Write the processed dataframe to an output file.

      :param df: Dataframe to write.
                 If not provided, runs `self.process()`
      :type df: Optional[pl.DataFrame]
      :param ext: What extension to write the file to
      :type ext: Literal["ndjson", "parquet", "arrow", "csv"]
      :param mode: Whether to output a single file (optimized) or maintain
                   the same file structure as the original files (robust).
                   Optimized should be faster, but robust should allow for bigger
                   data as we don't need all data in memory at the same time.
      :type mode: Literal['optimized', 'robust'], default = 'optimized'



   .. py:method:: create_mapping_file()

      Create a file to write the column mapping



   .. py:method:: load_hds_files()

      Load the historical dataset files from the `config.hds_folder` location.



   .. py:method:: read_predictor_type_from_file(df: polars.LazyFrame)

      Infer the types of the preditors from the data.

      This is non-trivial, as it's not ideal to pull in all data to memory for this.
      For this reason, we sample 1% of data, or all data if less than 50 rows,
      and try to cast it to numeric. If that fails, we set it to categorical,
      else we set it to numeric.

      It is technically supported to manually override this, by just overriding
      the `symbolic_predictors_to_mask` & `numeric_predictors_to_mask` properties.

      :param df: The lazyframe to infer the types with
      :type df: pl.LazyFrame



   .. py:method:: read_predictor_type_from_datamart(datamart_folder: pathlib.Path, datamart: pdstools.ADMDatamart = None)
      :staticmethod:


      The datamart contains type information about each predictor.
      This function extracts that information to infer types for the HDS.

      :param datamart_folder: The path to the datamart files
      :type datamart_folder: Path
      :param datamart: The direct ADMDatamart object
      :type datamart: ADMDatamart



   .. py:method:: get_columns_by_type()

      Get a list of columns for each type.



   .. py:method:: get_predictors_mapping()

      Map the predictor names to their anonymized form.



   .. py:method:: getHasher(cols, algorithm='xxhash', seed='random', seed_1=None, seed_2=None, seed_3=None)


   .. py:method:: process(strategy='eager', **kwargs)

      Anonymize the dataset.



.. py:class:: PegaDefaultTables

   .. py:class:: ADMModelSnapshot

      .. py:attribute:: pxApplication


      .. py:attribute:: pyAppliesToClass


      .. py:attribute:: pyModelID


      .. py:attribute:: pyConfigurationName


      .. py:attribute:: pySnapshotTime


      .. py:attribute:: pyIssue


      .. py:attribute:: pyGroup


      .. py:attribute:: pyName


      .. py:attribute:: pyChannel


      .. py:attribute:: pyDirection


      .. py:attribute:: pyTreatment


      .. py:attribute:: pyPerformance


      .. py:attribute:: pySuccessRate


      .. py:attribute:: pyResponseCount


      .. py:attribute:: pxObjClass


      .. py:attribute:: pzInsKey


      .. py:attribute:: pxInsName


      .. py:attribute:: pxSaveDateTime


      .. py:attribute:: pxCommitDateTime


      .. py:attribute:: pyExtension


      .. py:attribute:: pyActivePredictors


      .. py:attribute:: pyTotalPredictors


      .. py:attribute:: pyNegatives


      .. py:attribute:: pyPositives


      .. py:attribute:: pyRelativeNegatives


      .. py:attribute:: pyRelativePositives


      .. py:attribute:: pyRelativeResponseCount


      .. py:attribute:: pyMemory


      .. py:attribute:: pyPerformanceThreshold


      .. py:attribute:: pyCorrelationThreshold


      .. py:attribute:: pyPerformanceError


      .. py:attribute:: pyModelData


      .. py:attribute:: pyModelVersion


      .. py:attribute:: pyFactoryUpdatetime



   .. py:class:: ADMPredictorBinningSnapshot

      .. py:attribute:: pxCommitDateTime


      .. py:attribute:: pxSaveDateTime


      .. py:attribute:: pyModelID


      .. py:attribute:: pxObjClass


      .. py:attribute:: pzInsKey


      .. py:attribute:: pxInsName


      .. py:attribute:: pyPredictorName


      .. py:attribute:: pyContents


      .. py:attribute:: pyPerformance


      .. py:attribute:: pyPositives


      .. py:attribute:: pyNegatives


      .. py:attribute:: pyType


      .. py:attribute:: pyTotalBins


      .. py:attribute:: pyResponseCount


      .. py:attribute:: pyRelativePositives


      .. py:attribute:: pyRelativeNegatives


      .. py:attribute:: pyRelativeResponseCount


      .. py:attribute:: pyBinNegatives


      .. py:attribute:: pyBinPositives


      .. py:attribute:: pyBinType


      .. py:attribute:: pyBinNegativesPercentage


      .. py:attribute:: pyBinPositivesPercentage


      .. py:attribute:: pyBinSymbol


      .. py:attribute:: pyBinLowerBound


      .. py:attribute:: pyBinUpperBound


      .. py:attribute:: pyRelativeBinPositives


      .. py:attribute:: pyRelativeBinNegatives


      .. py:attribute:: pyBinResponseCount


      .. py:attribute:: pyRelativeBinResponseCount


      .. py:attribute:: pyBinResponseCountPercentage


      .. py:attribute:: pySnapshotTime


      .. py:attribute:: pyBinIndex


      .. py:attribute:: pyLift


      .. py:attribute:: pyZRatio


      .. py:attribute:: pyEntryType


      .. py:attribute:: pyExtension


      .. py:attribute:: pyGroupIndex


      .. py:attribute:: pyCorrelationPredictor



   .. py:class:: pyValueFinder

      .. py:attribute:: pyDirection


      .. py:attribute:: pySubjectType


      .. py:attribute:: ModelPositives


      .. py:attribute:: pyGroup


      .. py:attribute:: pyPropensity


      .. py:attribute:: FinalPropensity


      .. py:attribute:: pyStage


      .. py:attribute:: pxRank


      .. py:attribute:: pxPriority


      .. py:attribute:: pyModelPropensity


      .. py:attribute:: pyChannel


      .. py:attribute:: Value


      .. py:attribute:: pyName


      .. py:attribute:: StartingEvidence


      .. py:attribute:: pySubjectID


      .. py:attribute:: DecisionTime


      .. py:attribute:: pyTreatment


      .. py:attribute:: pyIssue



.. py:class:: ValueFinder(path: Optional[str] = None, df: Optional[Union[pandas.DataFrame, polars.DataFrame, polars.LazyFrame]] = None, verbose: bool = True, import_strategy: Literal['eager', 'lazy'] = 'eager', ncust: int = None, **kwargs)

   Class to analyze Value Finder datasets.

   Relies heavily on polars for faster reading and transformations.
   See https://pola-rs.github.io/polars/py-polars/html/index.html

   Requires either df or a path to be supplied,
   If a path is supplied, the 'filename' argument is optional.
   If path is given and no filename is, it will look for the most recent.

   :param path: Path to the ValueFinder data files
   :type path: Optional[str]
   :param df: Override to supply a dataframe instead of a file.
              Supports pandas or polars dataframes
   :type df: Optional[DataFrame]
   :param import_strategy: Whether to import the file fully to memory, or scan the file
                           When data fits into memory, 'eager' is typically more efficient
                           However, when data does not fit, the lazy methods typically allow
                           you to still use the data.
   :type import_strategy: Literal['eager', 'lazy'], default = 'eager'
   :param verbose: Whether to print out information during importing
   :type verbose: bool

   :keyword th: An optional keyword argument to override the propensity threshold
   :kwtype th: float
   :keyword filename: The name, or extended filepath, towards the file
   :kwtype filename: Optional[str]
   :keyword subset: Whether to select only a subset of columns.
                    Will speed up analysis and reduce unused information
   :kwtype subset: bool


   .. py:attribute:: import_strategy


   .. py:attribute:: keep_cols
      :value: ['pyStage', 'pyIssue', 'pyGroup', 'pyChannel', 'pyDirection', 'CustomerID', 'pyName',...



   .. py:attribute:: df


   .. py:attribute:: NBADStages
      :value: ['Eligibility', 'Applicability', 'Suitability', 'Arbitration']



   .. py:attribute:: StageOrder


   .. py:attribute:: maxPropPerCustomer


   .. py:attribute:: customersummary


   .. py:attribute:: countsPerStage


   .. py:attribute:: _thMap


   .. py:attribute:: countsPerThreshold


   .. py:method:: save_data(path: str = '.') -> os.PathLike

      Cache the ValueFinder dataset to a file

      :param path: Where to place the file
      :type path: str

      :returns: The paths to the file
      :rtype: PathLike



   .. py:method:: getCustomerSummary(th: Optional[float] = None) -> polars.DataFrame

      Computes the summary of propensities for all customers

      :param th: The threshold to consider an action 'good'.
                 If a customer has actions with propensity above this,
                 the customer has at least one relevant action.
                 If not given, will default to 5th quantile.
      :type th: Optional[float]



   .. py:method:: getCountsPerStage(customersummary: Optional[polars.DataFrame] = None) -> polars.DataFrame

      Generates an aggregated view per stage.

      :param customersummary: Optional override of the customer summary,
                              which can be generated by getCustomerSummary().
      :type customersummary: Optional[pl.DataFrame]



   .. py:method:: getThFromQuantile(quantile: float) -> float

      Return the propensity threshold corresponding to a given quantile

      If the threshold is already in `self._thMap`, simply gets it from there
      Otherwise, computes the threshold and then adds it to the map.

      :param quantile: The quantile to get the threshold for
      :type quantile: float



   .. py:method:: getCountsPerThreshold(th, return_df=False) -> Optional[polars.LazyFrame]


   .. py:method:: addCountsForThresholdRange(start, stop, step, method=Literal['threshold, quantile']) -> None

      Adds the counts per stage for a range of quantiles or thresholds.

      Once computed, the values are added to `.countsPerThreshold` so we
      only need to compute each value once.

      :param start: The starting of the range
      :type start: float
      :param stop: The end of the range
      :type stop: float
      :param step: The steps to compute between start and stop
      :type step: float
      :param method: Whether to get a range of thresholds directly or compute
                     the thresholds from their quantiles
      :type method: Literal["threshold", "quantile"]:



   .. py:method:: plotPropensityDistribution(sampledN: int = 10000) -> plotly.graph_objects.Figure

      Plots the distribution of the different propensities.

      For optimization reasons (storage for all points in a boxplot and
      time complexity for computing the distribution plot),
      we have to sample to a reasonable amount of data points.

      :param sampledN: The number of datapoints to sample
      :type sampledN: int, default = 10_000



   .. py:method:: plotPropensityThreshold(sampledN=10000, stage='Eligibility') -> plotly.graph_objects.Figure

      Plots the propensity threshold vs the different propensities.

      :param sampledN: The number of datapoints to sample
      :type sampledN: int, default = 10_000



   .. py:method:: plotPieCharts(start: float = None, stop: float = None, step: float = None, *, method: Literal['threshold', 'quantile'] = 'threshold', rounding: int = 3, th: Optional[float] = None) -> plotly.graph_objects.FigureWidget

      Plots pie charts showing the distribution of customers

      The pie charts each represent the fraction of customers with
      the color indicating whether they have sufficient relevant actions
      in that stage of the NBAD arbitration.

      If no values are provided for start, stop or step, the pie charts are
      shown using the default propensity threshold, as part of the Value Finder
      class.

      :param start: The starting of the range
      :type start: float
      :param stop: The end of the range
      :type stop: float
      :param step: The steps to compute between start and stop
      :type step: float

      :keyword method: Whether the range is computed based on the threshold directly
                       or based on the quantile of the propensity
      :kwtype method: Literal['threshold', 'quantile'], default='threshold'
      :keyword rounding: The number of digits to round the values by
      :kwtype rounding: int
      :keyword th: Choose a specific propensity threshold to plot
      :kwtype th: Optional[float]



   .. py:method:: plotDistributionPerThreshold(start: float = None, stop: float = None, step: float = None, *, method: Literal['threshold', 'quantile'] = 'threshold', rounding=3) -> plotly.graph_objects.FigureWidget

      Plots the distribution of customers per threshold, per stage.

      Based on the precomputed data in self.countsPerThreshold,
      this function will plot the distribution per stage.

      To add more data points between a given range,
      simply pass all three arguments to this function:
      start, stop and step.

      :param start: The starting of the range
      :type start: float
      :param stop: The end of the range
      :type stop: float
      :param step: The steps to compute between start and stop
      :type step: float

      :keyword method: Whether the range is computed based on the threshold directly
                       or based on the quantile of the propensity
      :kwtype method: Literal['threshold', 'quantile'], default='threshold'
      :keyword rounding: The number of digits to round the values by
      :kwtype rounding: int



   .. py:method:: plotFunnelChart(level: str = 'Action', query=None, return_df=False, **kwargs)

      Plots the funnel of actions or issues per stage.

      :param level: Which element to plot:
                    - If 'Actions', plots the distribution of actions.
                    - If 'Issues', plots the distribution of issues
      :type level: str, default = 'Actions'



.. py:class:: Sample(ldf: polars.LazyFrame)

   .. py:attribute:: _ldf


   .. py:method:: sample(n)


   .. py:method:: height()


   .. py:method:: shape()


   .. py:method:: item()


.. py:class:: DecisionData(raw_data: polars.LazyFrame)

   Container data class for the raw decision data. Only one instance of this
   should exist and will be associated with the streamlit app state.

   It will keep a pointer to the raw interaction level data (as a
   lazy frame) but also has VBD-style aggregation(s) to speed things up.


   .. py:attribute:: unfiltered_raw_decision_data
      :type:  polars.LazyFrame
      :value: None



   .. py:attribute:: decision_data
      :type:  polars.LazyFrame
      :value: None



   .. py:attribute:: preaggregated_decision_data_filterview
      :type:  polars.LazyFrame
      :value: None



   .. py:attribute:: preaggregated_decision_data_remainingview
      :type:  polars.LazyFrame
      :value: None



   .. py:attribute:: fields_for_data_filtering
      :value: ['pxDecisionTime', 'pyConfigurationName', 'pyChannel', 'pyDirection', 'pyIssue', 'pyGroup',...



   .. py:attribute:: plot


   .. py:attribute:: extract_type


   .. py:attribute:: raw_data


   .. py:attribute:: available_columns


   .. py:attribute:: preaggregation_columns


   .. py:attribute:: max_win_rank
      :value: 5



   .. py:attribute:: AvailableNBADStages
      :value: ['Arbitration']



   .. py:property:: stages_from_arbitration_down
      All stages in the filter view starting at Arbitration. This initially
      will just be [Arbitration, Final] but as we get more stages in there
      may be more here.


   .. py:property:: arbitration_stage


   .. py:method:: _invalidate_cached_properties()

      Resets the properties of the class



   .. py:method:: applyGlobalDataFilters(filters: Optional[Union[polars.Expr, List[polars.Expr]]] = None)

      Apply a global set of filters



   .. py:method:: resetGlobalDataFilters()


   .. py:property:: getPreaggregatedFilterView
      Pre-aggregates the full dataset over customers and interactions providing
      a view of what is filtered at a stage.

      This pre-aggregation is pretty similar to what "VBD" does to interaction
      history. It aggregates over individual customers and interactions giving
      summary statistics that are sufficient to drive most of the analyses
      (but not all). The results of this pre-aggregation are much smaller
      than the original data and is expected to easily fit in memory. We therefore
      use polars caching to efficiently cache this.

      This "filter" view keeps the same organization as the decision analyzer data
      in that it records the actions that get filtered out at stages. From this
      a "remaining" view is easily derived.


   .. py:property:: getPreaggregatedRemainingView
      Pre-aggregates the full dataset over customers and interactions providing a view of remaining offers.

      This pre-aggregation builds on the filter view and aggregates over
      the stages remaining.


   .. py:property:: sample


   .. py:method:: getAvailableFieldsForFiltering(categoricalOnly=False)


   .. py:method:: cleanup_raw_data(df: polars.LazyFrame)

      This method cleans up the raw data we read from parquet/S3/whatever.

      This likely needs to change as and when we get closer to product, to
      match what comes out of Pega. It does some modest type casting and
      potentially changing back some of the temporary column names that have
      been added to generate more data.



   .. py:method:: getPossibleScopeValues()


   .. py:method:: getPossibleStageValues()


   .. py:method:: getDistributionData(stage: str, grouping_levels: List[str], trend=False, additional_filters: Optional[Union[polars.Expr, List[polars.Expr]]] = None) -> polars.LazyFrame


   .. py:method:: getFunnelData(level, additional_filters: Optional[Union[polars.Expr, List[polars.Expr]]] = None) -> polars.LazyFrame


   .. py:method:: getFilterComponentData(top_n, additional_filters: Optional[Union[polars.Expr, List[polars.Expr]]] = None) -> polars.DataFrame


   .. py:method:: reRank(additional_filters: Optional[Union[polars.Expr, List[polars.Expr]]] = None, overrides: List[polars.Expr] = []) -> polars.LazyFrame

      Calculates prio and rank for all PVCL combinations



   .. py:method:: get_win_loss_distribution_data(level, win_rank)


   .. py:property:: get_optionality_data
      Finding the average number of actions per stage without trend analysis.
      We have to go back to the interaction level data, no way to
      use pre-aggregations unfortunately.


   .. py:property:: get_optionality_data_with_trend
      Finding the average number of actions per stage with trend analysis.
      We have to go back to the interaction level data, no way to
      use pre-aggregations unfortunately.


   .. py:method:: getActionVariationData(stage)


   .. py:method:: getABTestResults()


   .. py:method:: getThresholdingData(fld, quantile_range=range(10, 100, 10))


   .. py:method:: getValueDistributionData()


   .. py:method:: aggregate_remaining_per_stage(df: polars.LazyFrame, group_by_columns: List[str], aggregations: List = []) -> polars.LazyFrame

      Workhorse function to convert the raw Decision Analyzer data (filter view) to
      the aggregates remaining per stage. Used all over the place.



   .. py:method:: get_offer_quality(df, group_by)

      Given a dataframe with filtered action counts at stages.
      Flips it to usual VF view by doing a rolling sum over stages.

      :param df: Decision Analyzer style filtered action counts dataframe.
      :type df: pl.LazyFrame
      :param groupby_cols: The list of column names to group by(["pxEngagementStage", "pxInteractionID"]).
      :type groupby_cols: list

      :returns: Value Finder style, available action counts per group_by category
      :rtype: pl.LazyFrame



   .. py:property:: get_overview_stats
      Creates an overview from sampled data


   .. py:method:: get_sensitivity(win_rank=1, filters=None)


   .. py:method:: get_offer_variability_stats(stage)


   .. py:method:: get_winning_or_losing_interactions(win_rank, group_filter, win: bool)


   .. py:method:: winning_from(interactions, win_rank, groupby_cols, top_k)


   .. py:method:: losing_to(interactions, win_rank, groupby_cols, top_k)


.. py:data:: __reports__

