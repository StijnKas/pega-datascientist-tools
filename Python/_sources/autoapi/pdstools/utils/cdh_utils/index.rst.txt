:py:mod:`pdstools.utils.cdh_utils`
==================================

.. py:module:: pdstools.utils.cdh_utils

.. autoapi-nested-parse::

   cdhtools: Data Science add-ons for Pega.

   Various utilities to access and manipulate data from Pega for purposes
   of data analysis, reporting and monitoring.



Module Contents
---------------


Functions
~~~~~~~~~

.. autoapisummary::

   pdstools.utils.cdh_utils.defaultPredictorCategorization
   pdstools.utils.cdh_utils._extract_keys
   pdstools.utils.cdh_utils.parsePegaDateTimeFormats
   pdstools.utils.cdh_utils.getTypeMapping
   pdstools.utils.cdh_utils.set_types
   pdstools.utils.cdh_utils.inferTableDefinition
   pdstools.utils.cdh_utils.safe_range_auc
   pdstools.utils.cdh_utils.auc_from_probs
   pdstools.utils.cdh_utils.auc_from_bincounts
   pdstools.utils.cdh_utils.aucpr_from_probs
   pdstools.utils.cdh_utils.aucpr_from_bincounts
   pdstools.utils.cdh_utils.auc2GINI
   pdstools.utils.cdh_utils._capitalize
   pdstools.utils.cdh_utils._polarsCapitalize
   pdstools.utils.cdh_utils.fromPRPCDateTime
   pdstools.utils.cdh_utils.toPRPCDateTime
   pdstools.utils.cdh_utils.weighted_average_polars
   pdstools.utils.cdh_utils.weighted_performance_polars
   pdstools.utils.cdh_utils.zRatio
   pdstools.utils.cdh_utils.lift
   pdstools.utils.cdh_utils.LogOdds
   pdstools.utils.cdh_utils.featureImportance
   pdstools.utils.cdh_utils.gains_table
   pdstools.utils.cdh_utils.legend_color_order
   pdstools.utils.cdh_utils.sync_reports



.. py:function:: defaultPredictorCategorization(x: Union[str, polars.Expr] = pl.col('PredictorName')) -> polars.Expr

   Function to determine the 'category' of a predictor.

   It is possible to supply a custom function.
   This function can accept an optional column as input
   And as output should be a Polars expression.
   The most straight-forward way to implement this is with
   pl.when().then().otherwise(), which you can chain.

   By default, this function returns "Primary" whenever
   there is no '.' anywhere in the name string,
   otherwise returns the first string before the first period

   :param x: The column to parse
   :type x: Union[str, pl.Expr], default = pl.col('PredictorName')


.. py:function:: _extract_keys(df: pdstools.utils.types.any_frame, col='Name', capitalize=True, import_strategy='eager') -> pdstools.utils.types.any_frame

   Extracts keys out of the pyName column

   This is not a lazy operation as we don't know the possible keys
   in advance. For that reason, we select only the pyName column,
   extract the keys from that, and then collect the resulting dataframe.
   This dataframe is then joined back to the original dataframe.

   This is relatively efficient, but we still do need the whole
   pyName column in memory to do this, so it won't work completely
   lazily from e.g. s3. That's why it only works with eager mode.

   :param df: The dataframe to extract the keys from
   :type df: Union[pl.DataFrame, pl.LazyFrame]


.. py:function:: parsePegaDateTimeFormats(timestampCol='SnapshotTime', timestamp_fmt: str = None, strict_conversion: bool = True)

   Parses Pega DateTime formats.

   Supports the two most commonly used formats:

   - "%Y-%m-%d %H:%M:%S"
   - "%Y%m%dT%H%M%S.%f %Z"

   If you want to parse a different timezone, then

   Removes timezones, and rounds to seconds, with a 'ns' time unit.

   :param timestampCol: The column to parse
   :type timestampCol: str, default = 'SnapshotTime'
   :param timestamp_fmt: An optional format to use rather than the default formats
   :type timestamp_fmt: str, default = None
   :param strict_conversion: Whether to error on incorrect parses or just return Null values
   :type strict_conversion: bool, default = True


.. py:function:: getTypeMapping(df, definition, verbose=False, **timestamp_opts)

   This function is used to convert the data types of columns in a DataFrame to a desired types.
   The desired types are defined in a `PegaDefaultTables` class.

   :param df: The DataFrame whose columns' data types need to be converted.
   :type df: pl.LazyFrame
   :param definition: A `PegaDefaultTables` object that contains the desired data types for the columns.
   :type definition: PegaDefaultTables
   :param verbose: If True, the function will print a message when a column is not in the default table schema.
   :type verbose: bool
   :param timestamp_opts: Additional arguments for timestamp parsing.
   :type timestamp_opts: str

   :returns: A list with polars expressions for casting data types.
   :rtype: List


.. py:function:: set_types(df, table='infer', verbose=False, **timestamp_opts)


.. py:function:: inferTableDefinition(df)


.. py:function:: safe_range_auc(auc: float) -> float

   Internal helper to keep auc a safe number between 0.5 and 1.0 always.

   :param auc: The AUC (Area Under the Curve) score
   :type auc: float

   :returns: 'Safe' AUC score, between 0.5 and 1.0
   :rtype: float


.. py:function:: auc_from_probs(groundtruth: List[int], probs: List[float]) -> List[float]

   Calculates AUC from an array of truth values and predictions.
   Calculates the area under the ROC curve from an array of truth values and
   predictions, making sure to always return a value between 0.5 and 1.0 and
   returns 0.5 when there is just one groundtruth label.

   :param groundtruth: The 'true' values, Positive values must be represented as
                       True or 1. Negative values must be represented as False or 0.
   :type groundtruth: List[int]
   :param probs: The predictions, as a numeric vector of the same length as groundtruth
   :type probs: List[float]
   :param Returns: The AUC as a value between 0.5 and 1.
   :type Returns: List[float]
   :param Examples: >>> auc_from_probs( [1,1,0], [0.6,0.2,0.2])


.. py:function:: auc_from_bincounts(pos: List[int], neg: List[int], probs: List[float] = None) -> float

   Calculates AUC from counts of positives and negatives directly
   This is an efficient calculation of the area under the ROC curve directly from an array of positives
   and negatives. It makes sure to always return a value between 0.5 and 1.0
   and will return 0.5 when there is just one groundtruth label.

   :param pos: Vector with counts of the positive responses
   :type pos: List[int]
   :param neg: Vector with counts of the negative responses
   :type neg: List[int]
   :param probs: Optional list with probabilities which will be used to set the order of the bins. If missing defaults to pos/(pos+neg).
   :type probs: List[float]

   :returns: * *float* -- The AUC as a value between 0.5 and 1.
             * *Examples* -- >>> auc_from_bincounts([3,1,0], [2,0,1])


.. py:function:: aucpr_from_probs(groundtruth: List[int], probs: List[float]) -> List[float]

   Calculates PR AUC (precision-recall) from an array of truth values and predictions.
   Calculates the area under the PR curve from an array of truth values and
   predictions. Returns 0.0 when there is just one groundtruth label.

   :param groundtruth: The 'true' values, Positive values must be represented as
                       True or 1. Negative values must be represented as False or 0.
   :type groundtruth: List[int]
   :param probs: The predictions, as a numeric vector of the same length as groundtruth
   :type probs: List[float]
   :param Returns: The AUC as a value between 0.5 and 1.
   :type Returns: List[float]
   :param Examples: >>> auc_from_probs( [1,1,0], [0.6,0.2,0.2])


.. py:function:: aucpr_from_bincounts(pos: List[int], neg: List[int], probs: List[float] = None) -> float

   Calculates PR AUC (precision-recall) from counts of positives and negatives directly.
   This is an efficient calculation of the area under the PR curve directly from an
   array of positives and negatives. Returns 0.0 when there is just one
   groundtruth label.

   :param pos: Vector with counts of the positive responses
   :type pos: List[int]
   :param neg: Vector with counts of the negative responses
   :type neg: List[int]
   :param probs: Optional list with probabilities which will be used to set the order of the bins. If missing defaults to pos/(pos+neg).
   :type probs: List[float]

   :returns: * *float* -- The PR AUC as a value between 0.0 and 1.
             * *Examples* -- >>> aucpr_from_bincounts([3,1,0], [2,0,1])


.. py:function:: auc2GINI(auc: float) -> float

   Convert AUC performance metric to GINI

   :param auc: The AUC (number between 0.5 and 1)
   :type auc: float

   :returns: * *float* -- GINI metric, a number between 0 and 1
             * *Examples* -- >>> auc2GINI(0.8232)


.. py:function:: _capitalize(fields: list) -> list

   Applies automatic capitalization, aligned with the R couterpart.

   :param fields: A list of names
   :type fields: list

   :returns: **fields** -- The input list, but each value properly capitalized
   :rtype: list


.. py:function:: _polarsCapitalize(df: polars.LazyFrame)


.. py:function:: fromPRPCDateTime(x: str, return_string: bool = False) -> Union[datetime.datetime, str]

   Convert from a Pega date-time string.

   :param x: String of Pega date-time
   :type x: str
   :param return_string: If True it will return the date in string format. If
                         False it will return in datetime type
   :type return_string: bool, default=False

   :returns: * *Union[datetime.datetime, str]* -- The converted date in datetime format or string.
             * *Examples* -- >>> fromPRPCDateTime("20180316T134127.847 GMT")
               >>> fromPRPCDateTime("20180316T134127.847 GMT", True)
               >>> fromPRPCDateTime("20180316T184127.846")
               >>> fromPRPCDateTime("20180316T184127.846", True)


.. py:function:: toPRPCDateTime(dt: datetime.datetime) -> str

   Convert to a Pega date-time string

   :param x: A datetime object
   :type x: datetime.datetime

   :returns: * *str* -- A string representation in the format used by Pega
             * *Examples* -- >>> toPRPCDateTime(datetime.datetime.now())


.. py:function:: weighted_average_polars(vals: Union[str, polars.Expr], weights: Union[str, polars.Expr]) -> polars.Expr


.. py:function:: weighted_performance_polars() -> polars.Expr

   Polars function to return a weighted performance


.. py:function:: zRatio(posCol: polars.Expr = pl.col('BinPositives'), negCol: polars.Expr = pl.col('BinNegatives')) -> polars.Expr

   Calculates the Z-Ratio for predictor bins.

   The Z-ratio is a measure of how the propensity in a bin differs from the average,
   but takes into account the size of the bin and thus is statistically more relevant.
   It represents the number of standard deviations from the avreage,
   so centers around 0. The wider the spread, the better the predictor is.

   To recreate the OOTB ZRatios from the datamart, use in a group_by.
   See `examples`.

   :param posCol: The (Polars) column of the bin positives
   :type posCol: pl.Expr
   :param negCol: The (Polars) column of the bin positives
   :type negCol: pl.Expr

   .. rubric:: Examples

   >>> df.group_by(['ModelID', 'PredictorName']).agg([zRatio()]).explode()


.. py:function:: lift(posCol: polars.Expr = pl.col('BinPositives'), negCol: polars.Expr = pl.col('BinNegatives')) -> polars.Expr

   Calculates the Lift for predictor bins.

   The Lift is the ratio of the propensity in a particular bin over the average
   propensity. So a value of 1 is the average, larger than 1 means higher
   propensity, smaller means lower propensity.

   :param posCol: The (Polars) column of the bin positives
   :type posCol: pl.Expr
   :param negCol: The (Polars) column of the bin positives
   :type negCol: pl.Expr

   .. rubric:: Examples

   >>> df.group_by(['ModelID', 'PredictorName']).agg([lift()]).explode()


.. py:function:: LogOdds(Positives=pl.col('Positives'), Negatives=pl.col('ResponseCount') - pl.col('Positives'))


.. py:function:: featureImportance(over=['PredictorName', 'ModelID'])


.. py:function:: gains_table(df, value: str, index=None, by=None)

   Calculates cumulative gains from any data frame.

   The cumulative gains are the cumulative values expressed
   as a percentage vs the size of the population, also expressed
   as a percentage.

   :param df: The (Polars) dataframe with the raw values
   :type df: pl.DataFrame
   :param value: The name of the field with the values (plotted on y-axis)
   :type value: str
   :param index = None: Optional name of the field for the x-axis. If not passed in
                        all records are used and weighted equally.
   :param by = None: Grouping field(s), can also be None

   :returns: A (Polars) dataframe with cum_x and cum_y columns and optionally
             the grouping column(s). Values for cum_x and cum_y are relative
             so expressed as values 0-1.
   :rtype: pl.DataFrame

   .. rubric:: Examples

   >>> gains_data = gains_table(df, 'ResponseCount', by=['Channel','Direction])


.. py:function:: legend_color_order(fig)

   Orders legend colors alphabetically in order to provide pega color
   consistency among different categories


.. py:function:: sync_reports(checkOnly: bool = False, autoUpdate: bool = False)

   Compares the report files in your local directory to the repo

   If any of the files are different from the ones in GitHub,
   will prompt you to update them.

   :param checkOnly: If True, only checks, does not prompt to update
   :type checkOnly: bool, default = False
   :param autoUpdate: If True, doensn't prompt for update and goes ahead
   :type autoUpdate: bool, default = False


